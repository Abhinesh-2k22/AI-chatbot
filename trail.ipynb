{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader,DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data,glob=\"*.pdf\",loader_cls=PyPDFLoader)\n",
    "    \n",
    "    documents = loader.load()\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data=load_pdf('D:/abhi/programming/AI_chatbot/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500,chunk_overlap = 20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9874\n"
     ]
    }
   ],
   "source": [
    "text_chunks = text_split(extracted_data)\n",
    "print(len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hugging_face_embedding():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\abhi\\programming\\AI_chatbot\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "d:\\abhi\\programming\\AI_chatbot\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings= hugging_face_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_faiss(embeddings, text_chunks):\n",
    "    # Create a FAISS index and add the document vectors\n",
    "    faiss_store = FAISS.from_documents(text_chunks, embeddings)\n",
    "    return faiss_store\n",
    "\n",
    "# Assuming you've already split your documents into chunks\n",
    "# text_chunks = text_split(extracted_data)\n",
    "\n",
    "# Initialize the FAISS vector store\n",
    "vector_store = initialize_faiss(embeddings, text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.save_local(\"faiss_index\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FAISS index with the flag set to allow deserialization\n",
    "faiss_store = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is giving incomplete sentence as output\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "from langchain.vectorstores import FAISS\n",
    "import numpy as np\n",
    "\n",
    "# Load the GPT-Neo model and tokenizer\n",
    "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "\n",
    "def generate_response(prompt: str) -> str:\n",
    "    # Tokenize the input prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    # Generate the text with a larger max_length or max_new_tokens\n",
    "    gen_tokens = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "        max_length=550,  # Increase this to accommodate longer prompts\n",
    "        # Alternatively, use max_new_tokens to limit the length of the generated text\n",
    "        max_new_tokens=100,\n",
    "    )\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]\n",
    "    return gen_text\n",
    "\n",
    "def main(query: str, faiss_store: FAISS):\n",
    "    # Perform the search\n",
    "    docs = faiss_store.similarity_search(query, k=5)\n",
    "\n",
    "    # Extract the relevant context from the documents\n",
    "    context = \" \".join([doc.page_content for doc in docs])\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = f\"Context: {context}\\nQuestion: {query}\\n\\nProvide a concise and accurate answer based on the context above.\"\n",
    "\n",
    "    # Generate the response\n",
    "    response = generate_response(prompt)\n",
    "    print(\"Response: \", response)\n",
    "\n",
    "# Example query\n",
    "user_query = \"What is the purpose of lungs?\"\n",
    "main(user_query, faiss_store)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=200) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  Context: •Outline the anatomy of the blood supply to the lungs\n",
      "•Describe the pleura of the lungs and their function\n",
      "A major organ of the respiratory system, each lung houses structures of both the conducting and respiratory zones. The\n",
      "main function of the lungs is to perform the exchange of oxygen and carbon dioxide with air from the atmosphere. To this\n",
      "end, the lungs exchange respiratory gases across a very large epithelial surface area—about 70 square meters—that is highly\n",
      "permeable to gases. •Describe the process of internal respiration\n",
      "The purpose of the respiratory system is to perform gas exchange. Pulmonary ventilation provides air to the alveoli for this\n",
      "gas exchange process. At the respiratory membrane, where the alveolar and capillary walls meet, gases move across the\n",
      "membranes, with oxygen entering the bloodstream and carbon dioxide exiting. It is through this mechanism that blood is circulation is very important, as blood is required to transport oxygen from the lungs to other tissues throughout the body.\n",
      "The function of the pulmonary circulation is to aid in gas exchange. The pulmonary artery provides deoxygenated blood to\n",
      "the capillaries that form respiratory membranes with the alveoli, and the pulmonary veins return newly oxygenated blood\n",
      "to the heart for further transport throughout the body. The lungs are innervated by the parasympathetic and sympathetic The other major activity in the lungs is the process of respiration, the process of gas exchange. The function of respiration\n",
      "is to provide oxygen for use by body cells during cellular respiration and to eliminate carbon dioxide, a waste product of\n",
      "cellular respiration, from the body. In order for the exchange of oxygen and carbon dioxide to occur, both gases must be\n",
      "transported between the external and internal respiration sites. Although carbon dioxide is more soluble than oxygen in volume of the lungs. The greater the volume of the lungs, the lower the air pressure within the lungs.\n",
      "Pulmonary ventilation consists of the process of inspiration (or inhalation), where air enters the lungs, and expiration\n",
      "(or exhalation), where air leaves the lungs. During inspiration, the diaphragm and external intercostal muscles contract,\n",
      "causing the rib cage to expand and move outward, and expanding the thoracic cavity and lung volume. This creates a\n",
      "Question: What is the purpose of lungs?\n",
      "\n",
      "Provide a concise and accurate answer based on the context above.\n",
      "\n",
      "Explanation:\n",
      "\n",
      "The purpose of the lungs is to perform gas exchange. Pulmonary ventilation provides air to the alveoli for this\n",
      "gas exchange process. At the respiratory membrane, where the alveolar and capillary walls meet, gases move across the\n",
      "membranes, with oxygen entering the bloodstream and carbon dioxide exiting. It is through this mechanism that blood is circulation is very important, as blood is required to transport oxygen from the lungs to other tissues throughout the body.\n",
      "\n",
      "The function of the pulmonary circulation is to aid in gas exchange. The pulmonary artery provides deoxygenated blood to\n",
      "the capillaries that form respiratory membranes with the alveoli, and the pulmonary veins return newly oxygenated blood\n",
      "to the heart for further transport throughout the body. The lungs are innervated by the parasympathetic and sympathetic The other major activity in the lungs is the process of respiration, the process of gas exchange. The function of respiration...\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "from langchain.vectorstores import FAISS\n",
    "import numpy as np\n",
    "\n",
    "# Load the GPT-Neo model and tokenizer\n",
    "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "\n",
    "def generate_response(prompt: str) -> str:\n",
    "    gen_tokens = model.generate(\n",
    "        tokenizer(prompt, return_tensors=\"pt\").input_ids,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.85,\n",
    "        max_length=512,\n",
    "        max_new_tokens=200,\n",
    "    )\n",
    "    gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Basic post-processing\n",
    "    if not gen_text.endswith(('.', '!', '?')):\n",
    "        gen_text += '...'\n",
    "    \n",
    "    return gen_text\n",
    "\n",
    "def main(query: str, faiss_store: FAISS):\n",
    "    # Perform the search\n",
    "    docs = faiss_store.similarity_search(query, k=5)\n",
    "\n",
    "    # Extract the relevant context from the documents\n",
    "    context = \" \".join([doc.page_content for doc in docs])\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = f\"Context: {context}\\nQuestion: {query}\\n\\nProvide a concise and accurate answer based on the context above.\"\n",
    "\n",
    "    # Generate the response\n",
    "    response = generate_response(prompt)\n",
    "    print(\"Response: \", response)\n",
    "\n",
    "# Example query\n",
    "user_query = \"What is the purpose of lungs?\"\n",
    "main(user_query, faiss_store)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
